# LLM Service

This service provides code generation capabilities using the Deepseek Coder 1.3B model.

## Building and Running

### Build the Docker image
```bash
docker build -t dev-assistant-llm .
```

### Run the container
```bash
docker run -p 8000:8000 dev-assistant-llm
```

Note: On first run, the service will download the model files (~3GB). This may take several minutes depending on your internet connection.

## Testing

### Check service health
```bash
curl http://localhost:8000/health
```

### Generate code
```bash
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "Write a Python function to reverse a string",
       "temperature": 0.7
     }'
```

## API Reference

### POST /generate
Generate code based on a prompt.

Parameters:
- `prompt` (string): The input prompt describing the code you want to generate
- `temperature` (float, optional): Controls randomness in generation (0.0 = deterministic, 1.0 = more random). Default: 0.7
- `max_length` (int, optional): Maximum length of generated response. Default: 1024

### GET /health
Check service health and environment details.

## Technical Details
- Uses FastAPI for the API server
- Runs on CPU by default, with automatic GPU detection (CUDA/MPS) if available
- Model: deepseek-ai/deepseek-coder-1.3b-base